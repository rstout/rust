- Tokens don't have branching (e.g. <|>)
- can have token "literls", e.g. using the parsing API we can do:
    -> hex = symbol("0") ->> oneOf("xX") ->> digit*
- want to capture values
    -> digit* should return the string of digits it parses
- user and lib error messages, need line and column info
    -> use while loop instead of "for x in iter" so we can backtrack
- start simple, by parsing just decimals and additions
- Token, Lexer, Parser
- Token fns - oneOf, inRange, not (!)
- Parse fns - option/maybe (?), many (*), many1 (+)
- Success, Failure, Error

- Token: list of accepting states? ADT for types of accepting states
  (range vs list)
- Separate what goes in parser and what goes in Lex/Token
- Token should be a single char
    -> for now, to make it easier

- What is a token?
- ADTs
- Tokens are a chain of accepting states
- We get a char of input. What next?
- Lexer is DFA with chars -> tokens, Parser is DFA with tokens -> AST
- Rounds: first round, get rid of tokens that don't match first char.
  Second round get rid of tokens that don't continue to match with second
  input char, and so on
- Token type should not be an ADT?
- matchAndConsume, then filter on those that matched
    -> repeat until in terminal state and next char doesn't match
- Have reserved keywords be tokens?
    -> No
- Comments
- parse :: FilePath -> AST
- Consume: function that returns a function
    -> like the that Go lib
- Difference between tokens vs token directives
    -> token::Plus vs the function used to define the token (e.g.
       symbol("+"), or lit_char('+'))
- Is Tokenizing/Lexing ambiguous? i.e. can the same input tokenize as 2
  different tokens? Like, is this a case that will come up?
    -> I don't think it is
- Have to keep state
    -> When tokenizing an identifier, need to return the entire string
       identifier
    -> So token objects need a payload/str field
- The list of Tokens that we give to the tokenizer have type <T: Token>
    -> i.e. they implement the Token interface
- Think typeclasses
- Tokenizer/Lexer has function called Tokenize/Lex that takes in a list (?)
  of elements of type <T: Token>
    -> or maybe returns such a list?
- Token can contain its parsing directive
- How do we make the API simple? What would we like to do, and how would we
  like to do it?
    -> Do this (skeleton code)
- Token creators are symbol("c"), digit.+(), etc
    -> they return Tokens
- Tokens could be type parameterized with a user Token type
- Have to map token directive to user token type(s)
- Could map TD to fn: ~str -> <T>, where T is user token type
- LexCmd = TokenDirective
- for _ in enum { try to match, get list of matching tokens (filter), and
  try to continue to match }
- Implement a trait for you token type
    -> Want the other way around though (don't want MyTokenType to map to
       LexCmds, want LexCmds to map to Tokens)
    -> We are trying to contruct tokens
- In parser user code, need to be able to specify strings
    -> e.g. ident("if")
- Maybe do ident(Option<~str>)?
    -> No, there should be a better way
    -> Want behvior completely user defined
- Issue: how does the user use their tokens in the parsing API?
    -> Problem with constructors: The user wants to specify ident, not
       ident(str)
    -> maybe ident shouldn't be a token?
    -> How do we use tokens with constructors in the Parse API?
- Maybe Tokens shouldn't have constructors, but should be instead be a field
  in a struct, with payload another field in the stuct
    ->  stuct Token<T> {
            type: T; // This is the user token type
            payload: ~str;
        }
    -> makeToken<T>(myToken: T) -> Token<T>
        -> Or just have a constructor for Token (Token<T>::new(MyToken))
    -> This is more flexible
- Use the std::ops traits to overwrite +,-,!, etc.
- DFA should be a (generic) trait, and LexCmd should implement that trait
    -> User should see LexCmd in the API instead of DFA
- Lexer will take a list of tokens, extract all the lexcmds and make one big
  lexcmd (?)
- For LexCmds, implement the SHL trait (>>), so that we can chain lexCmds
  like: hex = symbol("0") >> oneOf(["X", "x"]) >> many1(digit)
- Can also implement the ! trait for negation of a LexCmd (useful for oneOf)
- Tokens are what the Lexer should be giving the user
    -> Input to the Lexer is (map: LexCmd -> <T>)
    -> Output is Token<T>
- Should lexCmds/DFAs return str or Tokens?
    -> lexCmd return Tokens, DFAs return str?
- LexGraph is internal representation
- LexCmd: fn (input: str/char) -> (<T>) -> LexGraph
- No backtracking for now
- Think about symbol('a') >> symbol('b')
- 'a', 'b' -> symbol('a') >> symbol('b') => LexCmd { val: "ab", done: true,
  accepts(any) => false }
- LexCmds can be a linked list
    -> which can be "implemented" by the consume function
- >> has to be aware of what types of LexCmds it is joining
    -> the accept and consume functions of each type should be easily
       accessible/useable
- instead of consume returning a new ~LexCmd, it could just modify val, done,
  etc.
    -> But what about chaining? Returning a ~LexCmd makes chaining easier
       (if not possible in the first place)
    -> Look into it
- consume can use get_next
- Parser combinator
- When calling get next, we set val of next to the built-up string so far
- should start skeleton coding
- Need to implement send for Token<T>
    -> Use channels eventually, for now just return a list of tokens
- Need a way to couple LexCmd terminal state(s) with user Tokens
    -> but still be able to combine LexCmds without reference to Tokens
    -> i.e. symbol('0') >> symbol('x')
- Maybe instead of (>>) we can just input LexCmds as a list?
    -> many and many1 can still be wrappers (as they don't join)
    -> I think solving the LexCmd/Token coupling issue is important tho
- Want to be able to COMPOSE LexCmds
    -> (>>) is our composition operator
- Let's first design the symbol() LexCmd, and then do (>>) for them
    -> Need to keep arch design in mind
- Might want to develop a good DFA API
    -> Can be used by both the Lexer and Parser libs
- LexCmds should only be wrappers around the type of function (symbol, many1,
  etc.) and the input to that function. Combined with the User token type,
  this user arg can be turned into a DFA
- DFA will be type parameterized, but we want to give it a stream of chars
  and want to get a Token<T> back
- DFA has nodes, does/not accept, transitions, terminal states, type of input,
  type of output
    -> 2 transition fns: 1 if accepts, 1 if not accepts
- How to handle failure?
    -> On every (char) input, we need to check for failure, success, or
       continuing to make progress
    -> Maybe DFA could send results on a chan, but if there is failure,
       DFA could just close the port it's receiving input on
    -> This would signal to the provider that a failure occured
    -> We may want to signal soft errors though, in the case where the caller
       might want to backtrack
    -> Though backtrack could (should?) be baked into the DFA
- Is it worth it to write a DFA lib?
    -> Going to do most of one for LexCmds anyway, should go all the way to
       reduce work in parser?
- DFANode
- How do we handle multiple modifiers? (e.g. Not and Many combo)
- Does LexCmd's accepts() fn make LexKind obsolete?
- Make accept based on the kind
- Modified isn't the most clear
- Leaning towards a list of LexCmds over using (>>)
- Should start organizing into user and lib modules
- Layers:
    1. User code
    2. Library code that user interacts with. Must be simple, clear and
       be easy to understand and use
    3. Internals
- In this case, its 1. User Code 2. Lexer 3. DFA
- Start thinking functionaly/ADT
- Need to clean up Lexer.rs
    -> Documentation
- DFA: keep track of consumed input
    -> DFAs are directed cyclic graphs, where every node has at most 2
       out-branches (either matches or doesn't)
    -> terminal states, which return/output a value. You don't immediately
       end if you end up in a terminal state, though.
- Let's start with the question: What is a DFA Node?
- Binary DFA: nodes only have 2 out-branches
    -> I think is is just DFA, no need to specify "Binary" DFA
- Maybe just have a bool flag for "is_accept_state"
- Accept states can still have out-branches
- Maybe not have a branch for match failure?
    -> In our case, match failure means only one case?
    -> But for backtracking, we may need this
    -> So implement no-match branch
    -> Can always point all of them to the same node
- No-match in an accept state is handled differently
    -> i.e. as success
- Our built-up string should be kept in the DFA struct, not the Node struct
- (?) Parameterize Nodes over 2 types: input and struct that holds accept
  function?
    -> We have to parameterize over input type, let's hold off on the other
- DFA holds mut ref to current node, which gets updated
- When no-match on accept state, need to know how to return a token
- Could have lexCmd wrapper to not add consumed input to the string
- Whats the flow?
    -> We give our DFA a char
    -> It either updates its state or "returns" a token or failure
- Make DFA a trait or enum?
    -> Even abstract it to type parameters?
    -> Make it abstract so both lexer and parser can use it
- Build up output, then on no-match on accept state, package it up and send
  if through a channel
- I think DFA should be a trait
- What is the bare minimum needed to specify a DFA and how it should function?
    -> User needs to give Node graph, Nodes need an accept function
- What does the start node do?
- Does Node need public successNode and failNode? Could the accepts function
  just return the appropriate node?
    -> Maybe returns Option<Node>, and None indicates failure?
    -> If DFA gets back Some(), then add input to collection
    -> Nodes need a way to tell DFA not to add to the collection
    -> So what is the return type of Node.accepts()?
    -> If DFA gets None, does that mean we've ended on success or fail?
        -> Depends whether it's an accept state or not
- If we're in an accepting state, we'll eventually get kicked out (e.g. we'll
  hit whitespace)
    -> So on accept state to non-accept state we stop and that's a token
    -> accept state to accept state is okay (e.g. "if" -> "ifg", both are
       valid tokens)
- Don't have successNode and failNode
    -> restricts to just 2 out branches/nodes
    -> instead have a function that takes input (char) and returns
       Option<Node>
    -> If we get None, then success if Node is accept Node, fail ow
    -> If success, need to return token type to construct Token
- Abstract this all into functions
- DFA provides an API to be used by the library, not the user
- DFA is used to say yes or no to a stream of input
- Longest match
- Give a char to the DFA. What could we get back?
    -> Failure
    -> A Token (success)
    -> Nothing (still need more input)
- DFA fn consume(input)
    -> Iterate over the char stream, continuously consuming, occasionally
       outputting tokens
- LexResult: Fail or Token
- Consume does what with it's input?
    -> gives it to the accepts fn of the current Node
- Let's think about DFA and Nodes in the abstract
- Type of accepting state
    -> Different types of accepting states (for the different types of tokens)
- On failure (for some nodes) go back to start node