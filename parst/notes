- Tokens don't have branching (e.g. <|>)
- can have token "literls", e.g. using the parsing API we can do:
    -> hex = symbol("0") ->> oneOf("xX") ->> digit*
- want to capture values
    -> digit* should return the string of digits it parses
- user and lib error messages, need line and column info
    -> use while loop instead of "for x in iter" so we can backtrack
- start simple, by parsing just decimals and additions
- Token, Lexer, Parser
- Token fns - oneOf, inRange, not (!)
- Parse fns - option/maybe (?), many (*), many1 (+)
- Success, Failure, Error

- Token: list of accepting states? ADT for types of accepting states
  (range vs list)
- Separate what goes in parser and what goes in Lex/Token
- Token should be a single char
    -> for now, to make it easier

- What is a token?
- ADTs
- Tokens are a chain of accepting states
- We get a char of input. What next?
- Lexer is DFA with chars -> tokens, Parser is DFA with tokens -> AST
- Rounds: first round, get rid of tokens that don't match first char.
  Second round get rid of tokens that don't continue to match with second
  input char, and so on
- Token type should not be an ADT?
- matchAndConsume, then filter on those that matched
    -> repeat until in terminal state and next char doesn't match
- Have reserved keywords be tokens?
    -> No
- Comments
- parse :: FilePath -> AST
- Consume: function that returns a function
    -> like the that Go lib
- Difference between tokens vs token directives
    -> token::Plus vs the function used to define the token (e.g.
       symbol("+"), or lit_char('+'))
- Is Tokenizing/Lexing ambiguous? i.e. can the same input tokenize as 2
  different tokens? Like, is this a case that will come up?
    -> I don't think it is
- Have to keep state
    -> When tokenizing an identifier, need to return the entire string
       identifier
    -> So token objects need a payload/str field
- The list of Tokens that we give to the tokenizer have type <T: Token>
    -> i.e. they implement the Token interface
- Think typeclasses
- Tokenizer/Lexer has function called Tokenize/Lex that takes in a list (?)
  of elements of type <T: Token>
    -> or maybe returns such a list?
- Token can contain its parsing directive
- How do we make the API simple? What would we like to do, and how would we
  like to do it?
    -> Do this (skeleton code)
- Token creators are symbol("c"), digit.+(), etc
    -> they return Tokens
- Tokens could be type parameterized with a user Token type
- Have to map token directive to user token type(s)
- Could map TD to fn: ~str -> <T>, where T is user token type
- LexCmd = TokenDirective
- for _ in enum { try to match, get list of matching tokens (filter), and
  try to continue to match }
- Implement a trait for you token type
    -> Want the other way around though (don't want MyTokenType to map to
       LexCmds, want LexCmds to map to Tokens)
    -> We are trying to contruct tokens
- In parser user code, need to be able to specify strings
    -> e.g. ident("if")
- Maybe do ident(Option<~str>)?
    -> No, there should be a better way
    -> Want behvior completely user defined
- Issue: how does the user use their tokens in the parsing API?
    -> Problem with constructors: The user wants to specify ident, not
       ident(str)
    -> maybe ident shouldn't be a token?
    -> How do we use tokens with constructors in the Parse API?
- Maybe Tokens shouldn't have constructors, but should be instead be a field
  in a struct, with payload another field in the stuct
    ->  stuct Token<T> {
            type: T; // This is the user token type
            payload: ~str;
        }
    -> makeToken<T>(myToken: T) -> Token<T>
        -> Or just have a constructor for Token (Token<T>::new(MyToken))
    -> This is more flexible
- Use the std::ops traits to overwrite +,-,!, etc.
- DFA should be a (generic) trait, and LexCmd should implement that trait
    -> User should see LexCmd in the API instead of DFA
- Lexer will take a list of tokens, extract all the lexcmds and make one big
  lexcmd (?)
- For LexCmds, implement the SHL trait (>>), so that we can chain lexCmds
  like: hex = symbol("0") >> oneOf(["X", "x"]) >> many1(digit)
- Can also implement the ! trait for negation of a LexCmd (useful for oneOf)
- Tokens are what the Lexer should be giving the user
    -> Input to the Lexer is (map: LexCmd -> <T>)
    -> Output is Token<T>
- Should lexCmds/DFAs return str or Tokens?
    -> lexCmd return Tokens, DFAs return str?
- LexGraph is internal representation
- LexCmd: fn (input: str/char) -> (<T>) -> LexGraph
- No backtracking for now
- Think about symbol('a') >> symbol('b')
- 'a', 'b' -> symbol('a') >> symbol('b') => LexCmd { val: "ab", done: true,
  accepts(any) => false }
- LexCmds can be a linked list
    -> which can be "implemented" by the consume function
- >> has to be aware of what types of LexCmds it is joining
    -> the accept and consume functions of each type should be easily
       accessible/useable
- instead of consume returning a new ~LexCmd, it could just modify val, done,
  etc.
    -> But what about chaining? Returning a ~LexCmd makes chaining easier
       (if not possible in the first place)
    -> Look into it
- consume can use get_next
- Parser combinator
- When calling get next, we set val of next to the built-up string so far
- should start skeleton coding
- Need to implement send for Token<T>
    -> Use channels eventually, for now just return a list of tokens
- Need a way to couple LexCmd terminal state(s) with user Tokens
    -> but still be able to combine LexCmds without reference to Tokens
    -> i.e. symbol('0') >> symbol('x')
- Maybe instead of (>>) we can just input LexCmds as a list?
    -> many and many1 can still be wrappers (as they don't join)
    -> I think solving the LexCmd/Token coupling issue is important tho
- Want to be able to COMPOSE LexCmds
    -> (>>) is our composition operator
- Let's first design the symbol() LexCmd, and then do (>>) for them
    -> Need to keep arch design in mind
- Might want to develop a good DFA API
    -> Can be used by both the Lexer and Parser libs
- LexCmds should only be wrappers around the type of function (symbol, many1,
  etc.) and the input to that function. Combined with the User token type,
  this user arg can be turned into a DFA
- DFA will be type parameterized, but we want to give it a stream of chars
  and want to get a Token<T> back
- DFA has nodes, does/not accept, transitions, terminal states, type of input,
  type of output
    -> 2 transition fns: 1 if accepts, 1 if not accepts
- How to handle failure?
    -> On every (char) input, we need to check for failure, success, or
       continuing to make progress
    -> Maybe DFA could send results on a chan, but if there is failure,
       DFA could just close the port it's receiving input on
    -> This would signal to the provider that a failure occured
    -> We may want to signal soft errors though, in the case where the caller
       might want to backtrack
    -> Though backtrack could (should?) be baked into the DFA
- Is it worth it to write a DFA lib?
    -> Going to do most of one for LexCmds anyway, should go all the way to
       reduce work in parser?
- DFANode
- How do we handle multiple modifiers? (e.g. Not and Many combo)
- Does LexCmd's accepts() fn make LexKind obsolete?
- Make accept based on the kind
- Modified isn't the most clear
- Leaning towards a list of LexCmds over using (>>)
- Should start organizing into user and lib modules
- Layers:
    1. User code
    2. Library code that user interacts with. Must be simple, clear and
       be easy to understand and use
    3. Internals
- In this case, its 1. User Code 2. Lexer 3. DFA
- Start thinking functionaly/ADT
- Need to clean up Lexer.rs
    -> Documentation
- DFA: keep track of consumed input
    -> DFAs are directed cyclic graphs, where every node has at most 2
       out-branches (either matches or doesn't)
    -> terminal states, which return/output a value. You don't immediately
       end if you end up in a terminal state, though.
- Let's start with the question: What is a DFA Node?
- Binary DFA: nodes only have 2 out-branches
    -> I think is is just DFA, no need to specify "Binary" DFA
- Maybe just have a bool flag for "is_accept_state"
- Accept states can still have out-branches
- Maybe not have a branch for match failure?
    -> In our case, match failure means only one case?
    -> But for backtracking, we may need this
    -> So implement no-match branch
    -> Can always point all of them to the same node
- No-match in an accept state is handled differently
    -> i.e. as success
- Our built-up string should be kept in the DFA struct, not the Node struct
- (?) Parameterize Nodes over 2 types: input and struct that holds accept
  function?
    -> We have to parameterize over input type, let's hold off on the other
- DFA holds mut ref to current node, which gets updated
- When no-match on accept state, need to know how to return a token
- Could have lexCmd wrapper to not add consumed input to the string
- Whats the flow?
    -> We give our DFA a char
    -> It either updates its state or "returns" a token or failure
- Make DFA a trait or enum?
    -> Even abstract it to type parameters?
    -> Make it abstract so both lexer and parser can use it
- Build up output, then on no-match on accept state, package it up and send
  if through a channel
- I think DFA should be a trait
- What is the bare minimum needed to specify a DFA and how it should function?
    -> User needs to give Node graph, Nodes need an accept function
- What does the start node do?
- Does Node need public successNode and failNode? Could the accepts function
  just return the appropriate node?
    -> Maybe returns Option<Node>, and None indicates failure?
    -> If DFA gets back Some(), then add input to collection
    -> Nodes need a way to tell DFA not to add to the collection
    -> So what is the return type of Node.accepts()?
    -> If DFA gets None, does that mean we've ended on success or fail?
        -> Depends whether it's an accept state or not
- If we're in an accepting state, we'll eventually get kicked out (e.g. we'll
  hit whitespace)
    -> So on accept state to non-accept state we stop and that's a token
    -> accept state to accept state is okay (e.g. "if" -> "ifg", both are
       valid tokens)
- Don't have successNode and failNode
    -> restricts to just 2 out branches/nodes
    -> instead have a function that takes input (char) and returns
       Option<Node>
    -> If we get None, then success if Node is accept Node, fail ow
    -> If success, need to return token type to construct Token
- Abstract this all into functions
- DFA provides an API to be used by the library, not the user
- DFA is used to say yes or no to a stream of input
- Longest match
- Give a char to the DFA. What could we get back?
    -> Failure
    -> A Token (success)
    -> Nothing (still need more input)
- DFA fn consume(input)
    -> Iterate over the char stream, continuously consuming, occasionally
       outputting tokens
- LexResult: Fail or Token
- Consume does what with it's input?
    -> gives it to the accepts fn of the current Node
- Let's think about DFA and Nodes in the abstract
- Type of accepting state
    -> Different types of accepting states (for the different types of tokens)
- On failure (for some nodes) go back to start node
- DFA should be a generic library that the layer above queries/uses
    -> e.g. Are we in an accept state?
    -> What about returning Tokens?
    -> Allow querying for metadata, e.g. What /type/ of state are we at?
       i.e. in addition to querying about whether accept state
    -> In our implementation, all accept states should be marked with a
       token type (?)
- So first write DFA/Node impl without Lex/Parse in your head
    -> Then /add-to/ DFA API so it plays nice with Lex/Parse wants
- Architecture:
                                   User
                                Lexer/Parser
                     Bridge (between Lexer/Parser and DFA)
                                    DFA
- (?) "03" doesn't need to be taken care of in parse/lex phase, could be taken
  care of in checker?
- Have a node type that has no outbranches?
    -> For the generic case
    -> could be used to tell when we've stopped parsing a single token?
    -> could contain a token type?
* Building stuff from the input show be delegated to the level above
    -> To the Bridge
    -> The lexer/parser then uses the Bridge/DFA API
- DFA as an API
    -> behind an interface
- Separate querying for acceptance and going to next node
    -> Maybe have return type of accepts() be Option<Node>?
    -> But don't want to expose Nodes to user/caller
- Be sure to add good documentation
- How will the Lexer use the DFA API?
    -> lexer: new_lexer_dfa();
    -> Want to input char stream and get out token stream
- How do we constuct a DFA?
- Maybe have Nodes execute a function when they stop matching from a terminal
  state?
    -> So when we're "done" parsing an ident (e.g. we see a '+'), then we
       have some way to tell the Node to execute some function
    -> But we want the built up token to reside at the higher level, i.e.
       the lexer worries about building the token, not the DFA/Nodes
- Is there a case where we go from a terminal state to a non terminal state,
  and then back to a terminal state?
    -> If not, then a transition from terminal to non-terminal signifies
       a token parse (or failure?)
- How do we signal failure?
    -> The Bridge will worry about this, and will keep DFA construction in
       mind
    -> i.e. the way we construct the DFA will allow us to signal failure
       more easily
- The Bridge will be the one constructing tokens/AST-things and sending
  them over the channel
- The Bridge interface for Lexer and Parser should look the same?
- In constructing the DFA (i.e. creating Nodes), we can customize
    -> Add metadata (e.g. token type) to the Node
- Can't have T -> NT -> T because we construct our tokens (well, the token
  creation API) s.t. the only terminal state is at the end (i.e. no more
  input is expected)
    -> What about many(string("some"))?
    -> But then "e" to "s" is a match
- So we'll never have T -> no match -> NT -> T
- Leverage the fact that we can use the DFA to make Nodes that make sense to
  us (e.g. failure nodes, or using the start node as some signal)
- DFA isn't used to query for a match, it's used to query for the next node,
  and to query the state of the node
- Transitioning from a terminal state to the start state could signal
  that we've parsed a token
    -> Have to re-input the char that cause the transition to the start node,
       so that the start node can start to parse it as a token
- Improve and refine DFA/Node API
- Step 1: Turn unmodified lexcmds to Nodes
  Step 2: Turn modified lexcmds to Nodes
  Step 3: Turn arrays of lexcmds into linked Nodes
    -> For each node (in the chain), will have to add the start node to
       the unmatch links
- Make failNode into an option type instead of making it start node?
- For the case of many(), successNode points to itself and failNode points
  to ?
- When Lexer/Parse DFA/Node stuff is written, see what gets written twice,
  and try to make it so that it's only written once
- fn LexCmd -> LexNode
- fn [LexCmd] -> LexNode could be a map-reduce
- For non-Many nodes, want success_node to link to the next node. For Many
  node, want succes_node to link to itself, and fail_node to link to next
    -> Is this really what we want?
- I think we have to rethink things, clean them up
    -> Things are messy now, maybe because of poor planning
- Should metadata be held in a node?
    -> Maybe accept states should be own type, and can have metadata field
    -> Not every node needs a token type, so not every node should have a
       token type
- How do we return a token at the end of parsing a token?
    -> How do we know we're done?
    -> How do we do this cleanly?
    -> Formalize accept states?
- Separate accept state Node type
    -> cleans up logic and code?
- Make DFA/Nodes generic, and have constructing logic
- Make everything immutable/ephemeral (functional)
- Do we even need a fail node? If we don't match, then what?
    -> Inital thought: have it be an Option type, or return the start node
    -> Rethink: Don't need fail node
- Simple: make it a linked list
    -> Focus should be making things simple and clean
    -> Maybe just start with symbol(), and add from there?
    -> And should think about clearly defined modules from the beginning
- (?) Let's restart; clean slate
    -> git?
    -> so copy everything to a copy, and then modify the original dir
- Only worry about symbol
    -> Makes sense: thinking about too many things in inefficient
    -> think about them on a by-need basis
    -> Or make a good foundation, and then modify/build that foundation in
       small steps that are reasoned about appropriately
    -> They're small, so easy to reason about
- Do everything with symbol
    -> DFA/Nodes, Lexer, Parser
    -> Get something working
    -> And then worry about adding more stuff, like more commands and cmd
       modifiers (many1, etc)
    -> Start with 2 tokens: + and -
- Think about maps, folds and filters
- Could make (~[~LexCmd], T) its own type?
- Learn about lifetimes and ownership so we don't keep getting stuck on this
  stuff
- Think about token overlap
    -> e.g. between '+' and '++'
    -> either have to do backtracing, or combine them both (into one path)
    -> But let's stick to simple first, worry about that 0.1 release
- Make Parst its own github repo
- Release 0.1 should:
    -> Have excellent code quality (naming, modules, short functions, etc)
    -> Be well documented
    -> Have unit tests
- Start with minimum base functionality, and work up from there
- Immutable
    -> i.e. don't use the 'mut' keyword anywhere
- Other nodes could try parsing the already parsed string (if there is one),
  and if they "need" more to parse, get it from the iter
    -> What happens when input is consumed, but failure is reached?
    -> What do we do with the consumed input? With the iter?
- Start with mutable impl and then make it immutable?
- Is there a way to set an iterator to a certain position?
- Layer of indirection
    -> Have a dude pull from the iterator, and put into a list. Also, give
       that char to the node.
    -> If the node fails, instead of the dude pulling from the iterator,
       the dude will pull from the list
    -> When all chars have been pulled from the list, the dude will pull from
       the iterator again
    -> On match, the chars in the array are made into a string, and along
       with the token type from the match, a token is constructed and
       put into the token array (the return value from lex)
    -> If no match, turn array into string to show in error message
    -> "Failed to tokenize <array contents>"
- char_getter has methods next(), reset(), make_token()
    -> reset() resets index into the array
    -> keeps track of index into array
    -> grows array (with append())
- Need to keep track of longest, /Successful/ match (its token + str)
    -> Keep track of it in DFA?
    -> DFA will call char_getter.next() and pass it on to Node
    -> What happens when no more chars?
- Better name for CharGetter?
- May have leftover state in CharGettter after parsing a token
    -> e.g. the longest parsed token could be 5 chars long, but there could
       have been a token that we parsed 8 chars for, but which failed on the
       9th
- Going to have to test the shit out of this
- It would be awesome if I could make this all immutable and clean
    -> That's the goal: world class code (can't do world class design)
    -> Think harder about using map, filter, fold, scan etc.
- State
    -> token type + string (What we care about after the 2 folds)
    -> CharGetter (may need to retain state betweeen 2D folds)
- (?) Define fold for LexNode/LexDFA?
- Fold across
    -> Return either Some(Token<T>) or None, and a CharGetter
- Fold down
    -> Return Some(Token<T> with longest str) or None, and a CharGetter
- Fold down calls the fold acrosses
    -> Char getters need to be used serially, because there is only one
       underlying char iter that needs to be used serially
- CharGetter should be checked for exhuastion before every fold down
- A few edge cases to test
    -> CharGetter running out of input during fold across
    -> etc.
- Make TokenMake more self-contained
- Write tests
- Clean up token_maker.rs
    -> Better naming, try not to use mutability
- Let's do skeleton code
    -> Let's start from the beginning and rethink things
- Better name: TokenScanner, or just Scanner
    -> Make BufferedIter its own type/struct?
    -> Then TokenScanner consists of Option<TokenState> and a BufferedIter
- BufferedIter is more than just buffered
    -> we can move/pull from anywhere in the BufferedIter
- Could use RandomAccessIterator
    -> Still need to keep track of what has been parsed
    -> Could allow us to work with strings instead of [char]
    -> Would it make immutablitiy easier?
    -> Would only have to store indexes?
    -> *** RandomAccessIterator reduces state, because we don't have to worry
       about state modification due to next()
- How do we make while(!is_exhausted()) into functional/immutable goodness?
    -> recursion
- Look at rust src to see how they do doc
- Can't use RandomAccessIterator because we can't get a RAI from a File, can
  only get an Iterator<u8>
- We have an immutable interface that does mutation under the covers
    -> pretty cool
- Remember to do failure
    -> If none of the branches returned a token, do something
    -> possibly something with the partially parsed input